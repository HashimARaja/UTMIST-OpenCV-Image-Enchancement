{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To run Model all required code modules are marked with \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Global Variables *\n",
    "\n",
    "This notebook demonstrates various image enhancement techniques using OpenCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import model_selection\n",
    "from pathlib import Path\n",
    "\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "Dataset_path = \"E:/Github/UTMIST-OpenCV-Image-Enchancement/Dataset4K\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing that Data Exists and can be displayed\n",
    "\n",
    "We will start by loading an image from the disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image found\n"
     ]
    }
   ],
   "source": [
    "image = cv2.imread(\"dataset4k/4k-3840-x-2160-wallpapers-themefoxx (1).jpg\")\n",
    "\n",
    "if image is None:\n",
    "  print(\"image not found -- check to see if dataset path is correct or dataset is downloaded.\")\n",
    "else:\n",
    "  print(\"image found\")\n",
    "\n",
    "plt.imshow(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "\n",
    "half = cv2.resize(image, (0, 0), fx = 0.1, fy = 0.1)\n",
    "plt.imshow(cv2.cvtColor(half, cv2.COLOR_BGR2RGB))\n",
    "plt.title('Resized Image')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing and Creating a Training Dataset - i.e Downscaling Images - Test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in Path(Dataset_path).iterdir():\n",
    "  if each.is_file() and each.suffix in ['.jpg', '.jpeg', '.png']:\n",
    "    image = cv2.imread(str(each))\n",
    "    if image is not None:\n",
    "      downscaled_image = cv2.resize(image, (0, 0), fx=0.25, fy=0.25)\n",
    "      output_path = Path(\"E:/Github/UTMIST-OpenCV-Image-Enchancement/Downscaled_Dataset\") / each.name\n",
    "      cv2.imwrite(str(output_path), downscaled_image)\n",
    "      print(f\"Saved downscaled image to {output_path}\")\n",
    "    else:\n",
    "      print(f\"Failed to read image {each}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data into Tensors/Data Types - Decomissioned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2056 images from Downscaled_Dataset\n"
     ]
    }
   ],
   "source": [
    "def load_and_normalize_images(image_dir, normalize_range=(0, 1), target_size=(480, 270)):\n",
    "    image_paths = list(Path(image_dir).glob('*'))\n",
    "    images = []\n",
    "    \n",
    "    for image_path in image_paths:\n",
    "        image = cv2.imread(str(image_path))\n",
    "        if image is not None:\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            image = cv2.resize(image, target_size)\n",
    "            image = image.astype(np.float32)\n",
    "            if normalize_range == (0, 1):\n",
    "                image /= 255.0\n",
    "            elif normalize_range == (-1, 1):\n",
    "                image = (image / 127.5) - 1.0\n",
    "            images.append(image)\n",
    "        else:\n",
    "            print(f\"Failed to read image {image_path}\")\n",
    "    \n",
    "    return np.array(images)\n",
    "\n",
    "# Load and normalize images from Downscaled_Dataset\n",
    "downscaled_images = load_and_normalize_images(\"E:/Github/UTMIST-OpenCV-Image-Enchancement/Downscaled_Dataset\", normalize_range=(0, 1))\n",
    "print(f\"Loaded {len(downscaled_images)} images from Downscaled_Dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre Loading Data *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SuperResolutionDataset(Dataset):\n",
    "    def __init__(self, hr_dir, transform=None, downscale_factor=2):\n",
    "        \"\"\"\n",
    "        hr_dir: Directory containing the original 4K images.\n",
    "        transform: A torchvision.transforms pipeline for image conversion.\n",
    "        downscale_factor: Factor by which the image is downscaled to simulate low resolution.\n",
    "        \"\"\"\n",
    "        self.hr_dir = hr_dir\n",
    "        self.image_files = [os.path.join(hr_dir, file) for file in os.listdir(hr_dir)\n",
    "                            if file.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "        self.transform = transform\n",
    "        self.downscale_factor = downscale_factor\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load the high-resolution (HR) image (target)\n",
    "        hr_img = cv2.imread(self.image_files[index])\n",
    "        if hr_img is None:\n",
    "            raise ValueError(f\"Image not found or cannot be read: {self.image_files[index]}\")\n",
    "        hr_img = cv2.cvtColor(hr_img, cv2.COLOR_BGR2RGB)\n",
    "        h, w, _ = hr_img.shape\n",
    "\n",
    "        # Create the low-resolution (LR) image:\n",
    "        # 1. Downscale the HR image.\n",
    "        lr_img = cv2.resize(hr_img, (w // self.downscale_factor, h // self.downscale_factor),\n",
    "                            interpolation=cv2.INTER_CUBIC)\n",
    "        # 2. Upscale back to original resolution.\n",
    "        lr_img_upscaled = cv2.resize(lr_img, (w, h), interpolation=cv2.INTER_CUBIC)\n",
    "\n",
    "        if self.transform:\n",
    "            hr_img = self.transform(hr_img)\n",
    "            lr_img_upscaled = self.transform(lr_img_upscaled)\n",
    "\n",
    "        return lr_img_upscaled, hr_img\n",
    "\n",
    "# Define transforms: convert images to tensors.\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # You can add normalization here if needed.\n",
    "])\n",
    "\n",
    "# Specify your high-resolution images directory.\n",
    "hr_images_dir = r\"E:/Github/UTMIST-OpenCV-Image-Enchancement/Dataset4K\"\n",
    "dataset = SuperResolutionDataset(hr_dir=hr_images_dir, transform=transform, downscale_factor=2)\n",
    "dataloader = DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Architecture for Residual Blocks and 3x3 Conv. v ReLu Layer*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Residual Block: 3x3 Conv -> ReLU -> BatchNorm -> 3x3 Conv -> BatchNorm with skip connection.\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.relu  = nn.ReLU(inplace=True)\n",
    "        self.bn1   = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm2d(channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x  # Save input for the skip connection.\n",
    "        out = self.conv1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.bn1(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out += identity  # Skip connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Super-resolution network using residual blocks and PixelShuffle for upsampling.\n",
    "class SuperResolutionNet(nn.Module):\n",
    "    def __init__(self, num_channels=3, num_features=64, num_res_blocks=5, upscale_factor=2):\n",
    "        \"\"\"\n",
    "        num_channels: Number of channels in the image (3 for RGB).\n",
    "        num_features: Number of features after the initial convolution.\n",
    "        num_res_blocks: Number of residual blocks.\n",
    "        upscale_factor: Factor to upscale the input image.\n",
    "        \"\"\"\n",
    "        super(SuperResolutionNet, self).__init__()\n",
    "        \n",
    "        # Step 1: Feature Extraction\n",
    "        self.initial_conv = nn.Conv2d(num_channels, num_features, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        # Step 2: Residual Blocks\n",
    "        res_blocks = []\n",
    "        for _ in range(num_res_blocks):\n",
    "            res_blocks.append(ResidualBlock(num_features))\n",
    "        self.res_blocks = nn.Sequential(*res_blocks)\n",
    "        \n",
    "        # Step 3: Upsampling using sub-pixel convolution (PixelShuffle)\n",
    "        # For PixelShuffle, output channels of the conv must equal (upscale_factor^2 * num_features)\n",
    "        self.upsample_conv = nn.Conv2d(num_features, num_features * (upscale_factor ** 2), kernel_size=3, padding=1)\n",
    "        self.pixel_shuffle = nn.PixelShuffle(upscale_factor)\n",
    "        \n",
    "        # Step 4: Post-processing: Final convolution to refine output.\n",
    "        self.final_conv = nn.Conv2d(num_features, num_channels, kernel_size=3, padding=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Extract low-level features.\n",
    "        out = self.initial_conv(x)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        # Residual blocks for deep feature extraction.\n",
    "        out = self.res_blocks(out)\n",
    "        \n",
    "        # Upsampling.\n",
    "        out = self.upsample_conv(out)\n",
    "        out = self.pixel_shuffle(out)\n",
    "        \n",
    "        # Final refinement.\n",
    "        out = self.final_conv(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m device = \u001b[43mtorch\u001b[49m.device(\u001b[33m'\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      2\u001b[39m model = SuperResolutionNet(num_channels=\u001b[32m3\u001b[39m, num_features=\u001b[32m64\u001b[39m, num_res_blocks=\u001b[32m5\u001b[39m, upscale_factor=\u001b[32m2\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m criterion = nn.MSELoss()\n",
      "\u001b[31mNameError\u001b[39m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = SuperResolutionNet(num_channels=3, num_features=64, num_res_blocks=5, upscale_factor=2).to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "num_epochs = 10  # Set epochs as needed\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for lr_imgs, hr_imgs in dataloader:\n",
    "        lr_imgs = lr_imgs.to(device)\n",
    "        hr_imgs = hr_imgs.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(lr_imgs)\n",
    "        loss = criterion(outputs, hr_imgs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    avg_loss = epoch_loss / len(dataloader)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # (Optional) You could also calculate PSNR and SSIM here to monitor image quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start as the Python Environment 'Python 3.13.2' is no longer available. Consider selecting another kernel or refreshing the list of Python Environments."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available!\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
